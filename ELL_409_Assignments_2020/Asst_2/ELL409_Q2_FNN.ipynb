{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ELL409_Q2_FNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7hqJNyoopdA"
      },
      "source": [
        "import numpy as np\n",
        "#import math\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS8cvESnKirC"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZv0kzvS6uFF"
      },
      "source": [
        "# -------preprocessing of data ---------\n",
        "# x1 = list(range(len(traindata)))\n",
        "# np.random.shuffle(x1)\n",
        "# x2 = list(range(len(testdata)))\n",
        "# np.random.shuffle(x2)\n",
        "\n",
        "# traindata1 = traindata[np.argsort(x1)]\n",
        "# trainlabels1 = trainlabels[np.argsort(x1)]\n",
        "# testdata1 = testdata[np.argsort(x2)]\n",
        "# testlabels1 = testabels[np.argsort(x2)]\n",
        "\n",
        "# gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
        "# #np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
        "# # from PIL import Image\n",
        "\n",
        "# # # for td in traindata:\n",
        "# # td = traindata[10]\n",
        "# # print(td.T.shape)\n",
        "# # im = Image.fromarray(td.T)\n",
        "# # im.save('out.jpg')\n",
        "# # #im.convert('RGB').save('out.jpg')\n",
        "# #   #im.show()\n",
        "  \n",
        "# # td = np.dot(td.T,[0.2125, 0.7154, 0.0721])\n",
        "# #   # print(td[0])\n",
        "# # img = Image.fromarray(td)\n",
        "\n",
        "# # img.convert('RGB').save('out.png')\n",
        "\n",
        "# greydata = []\n",
        "# #i = 0\n",
        "# for td in testdata1:\n",
        "#     #print(td.shape)\n",
        "#     greydata.append(np.dot(td.T,[0.2125, 0.7154, 0.0721]).flatten())\n",
        "\n",
        "# greydata = np.asarray(greydata)\n",
        "\n",
        "# np.save(datapath+\"grey_testdata.npy\",greydata)\n",
        "# np.save(datapath+\"grey_testlabel.npy\",testlabels1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN31yhpmZ6u0"
      },
      "source": [
        "# # BATCH_SIZE = 4\n",
        "\n",
        "# # transform = transforms.ToTensor()\n",
        "\n",
        "# # traindata = torchvision.datasets.MNIST('/tmp', train=True, download=True, transform=transform)\n",
        "# # trainloader = torch.utils.data.DataLoader(traindata, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "# # testdata = torchvision.datasets.MNIST('/tmp', train=False, download=True, transform=transform)\n",
        "# # testloader = torch.utils.data.DataLoader(testdata, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "# dataiter = iter(testloader)\n",
        "\n",
        "# transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0,0,0), (1,1,1))]) #change normalize values if need be\n",
        "\n",
        "# # svhn = torchvision.datasets.SVHN('/tmp', download=True, transform=transform)\n",
        "# mnist = torchvision.datasets.MNIST('/tmp', download=True, transform=transform)\n",
        "\n",
        "# # svhn_loader = torch.utils.data.DataLoader(dataset=svhn,\n",
        "# #                                           batch_size=1,\n",
        "# #                                           shuffle=True,\n",
        "# #                                           num_workers=2)\n",
        "\n",
        "# mnist_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
        "#                                             batch_size=4,\n",
        "#                                             shuffle=True,\n",
        "#                                             num_workers=2)\n",
        "# dataiter = iter(mnist_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3xRH0nVObC_"
      },
      "source": [
        "datapath = \"/content/drive/MyDrive/A2_data/\" #already shuffled n flattened data\n",
        "traindata = np.load(datapath + \"grey_traindata.npy\", mmap_mode='r')\n",
        "trainlabels = np.load(datapath + \"grey_trainlabel.npy\", mmap_mode='r')\n",
        "testdata = np.load(datapath + \"grey_testdata.npy\", mmap_mode='r')\n",
        "testabels = np.load(datapath + \"grey_testlabel.npy\", mmap_mode='r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt2AfQhhGrBj"
      },
      "source": [
        "indxs = np.random.choice(len(traindata),int(len(traindata)/4))\n",
        "traindata = traindata[indxs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOGCQN4AJLe6"
      },
      "source": [
        "indxs = np.random.choice(len(testdata),int(len(testdata)/4))\n",
        "testdata = testdata[indxs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcnZk5of6RsD"
      },
      "source": [
        "if want MNIST, begin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy6_lOklZrvf"
      },
      "source": [
        "data = np.genfromtxt(\"/content/sample_data/mnist_train_small.csv\", skip_header=True,delimiter=\",\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liScLeb1bCEw"
      },
      "source": [
        "traindata = []\n",
        "trainlabels = []\n",
        "\n",
        "i = 0\n",
        "for val in data:\n",
        "  if(i<10000):\n",
        "    traindata.append(val[1:])\n",
        "    trainlabels.append(val[0])\n",
        "  i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LODdyytFbR0l"
      },
      "source": [
        "data = np.genfromtxt(\"/content/sample_data/mnist_test.csv\", skip_header=True,delimiter=\",\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPFw-2BTtc3p",
        "outputId": "27dbb1ac-dd63-4b04-8734-6120b514e46a"
      },
      "source": [
        "print(len(data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBTmqoBPbWiP"
      },
      "source": [
        "testdata = []\n",
        "testlabels = []\n",
        "i = 0\n",
        "for val in data:\n",
        "  if(i<5000):\n",
        "    testdata.append(val[1:])\n",
        "    testlabels.append(val[0])\n",
        "  i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSOpHG226UKc"
      },
      "source": [
        "mnist, end"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sys8Kk9a83hA"
      },
      "source": [
        "mu = np.mean(traindata,axis=0)\n",
        "std = np.std(traindata,axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa_NAlmu7qhy"
      },
      "source": [
        "traindata1 = (traindata - mu)/(std+10**-5) \n",
        "testdata1 = (testdata - mu)/(std+10**-5) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7APHV7-SHaA3"
      },
      "source": [
        "ncls = 10\n",
        "trainlabels1 = []\n",
        "for label in trainlabels:\n",
        "  v = np.zeros(shape=(ncls,))\n",
        "  #print(label)\n",
        "  v[int(label)] = 1\n",
        "  trainlabels1.append(v)\n",
        "  #print(v)\n",
        "\n",
        "#  break\n",
        "# trainlabels = np.asarray(trainlabels1)\n",
        "# del trainlabels1\n",
        "# print(trainlabels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7N2uz3RH375"
      },
      "source": [
        "testlabels1 = []\n",
        "for label in testlabels:\n",
        "  v = np.zeros(shape=(ncls,))\n",
        "  v[int(label)] = 1\n",
        "  testlabels1.append(v)\n",
        "\n",
        "# testabels = np.asarray(testlabels1)\n",
        "# del testlabels1\n",
        "# print(testabels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEYuW7QLNum8"
      },
      "source": [
        "univ_seed = 42\n",
        "np.random.seed(univ_seed)\n",
        "#reproducibility"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXEMkCqHKaMA"
      },
      "source": [
        "class layer:\n",
        "  #layer object - each is a FC layer \n",
        "\n",
        "  def __init__(self,num_in,num_out,weights=None,b=None,activation=None):\n",
        "    \n",
        "    self.shapeval = (num_in,num_out)\n",
        "    self.activation = activation\n",
        "    self.wts = np.random.randn(num_in,num_out) if weights is None else weights\n",
        "    self.bias = np.random.randn(num_out) if b is None else b\n",
        "    self.err = 0\n",
        "    self.diffval = 0\n",
        "    self.penalty_norm = None\n",
        "    self.activation_memo = None\n",
        "\n",
        "  def apply_activation(self,val,some_other=None):\n",
        "    if self.activation is None:\n",
        "      return val\n",
        "\n",
        "    #add activations inside class for speed\n",
        "    if self.activation == \"sigmoid\":\n",
        "      return 1/(1.0+np.exp(-val))\n",
        "    \n",
        "    if self.activation == \"softmax\":\n",
        "      e = np.exp(val)\n",
        "      return e / np.sum(e)\n",
        "\n",
        "    if self.activation == \"tanh\":\n",
        "      return np.tanh(val)\n",
        "\n",
        "    if self.activation == \"relu\":\n",
        "      return np.maximum(val, 0)\n",
        "\n",
        "    else: #undefined activation\n",
        "      if some_other is not None:\n",
        "        #of the form activation(input)\n",
        "        return some_other(val)\n",
        "\n",
        "    return val \n",
        "\n",
        "  def activation_derivative(self,val,some_other=None):\n",
        "    if self.activation == None:\n",
        "      return val\n",
        "    \n",
        "    if self.activation == \"sigmoid\":\n",
        "      tmp = 1/(1.0+np.exp(-val))\n",
        "      return tmp*(1-tmp)\n",
        "    \n",
        "    if self.activation == \"tanh\":\n",
        "      v = np.tanh(val)\n",
        "      return 1 - val**2\n",
        "    \n",
        "    if self.activation == \"softmax\":\n",
        "      e = np.exp(val)\n",
        "      softmax = e / np.sum(e)\n",
        "      s = softmax.reshape(1,-1)      \n",
        "      return np.diagflat(s) - np.dot(s, s.T)\n",
        "    \n",
        "    if self.activation == \"relu\":\n",
        "      return 1. * (val > 0)\n",
        "\n",
        "    else: #undefined activation\n",
        "      if some_other is not None:\n",
        "        #of the form activation_derivative(input)\n",
        "        return some_other(val)\n",
        "      \n",
        "    return val\n",
        "\n",
        "  def after_apply_activ(self,val):\n",
        "    \n",
        "    net_out = np.dot(val,self.wts) + self.bias\n",
        "    self.activation_memo = self.apply_activation(net_out)\n",
        "    return self.activation_memo\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rut7YVIlzPjN"
      },
      "source": [
        "class FNN:\n",
        "  #FNN object - describes a FNN/MLP\n",
        "  def __init__(self):\n",
        " \n",
        "    self.network = list()\n",
        "    #---better to make a diff layer class-----\n",
        "    # self.network.append(np.random.random(size=(num_feat,num_hide[0])))\n",
        "    # self.network_bias.append(np.ones(size=(num_hide[0],1)))\n",
        "\n",
        "    # for i in range(0,len(num_hide)-1):\n",
        "    #   self.network.append(np.random.random(size=(num_hide[i],num_hide[i+1])))\n",
        "    #   self.network_bias.append(np.ones(size=(num_hide[i+1],1)))\n",
        "    \n",
        "    # self.network.append(np.random.random(size=(num_hide[-1],num_out)))\n",
        "    # self.network_bias.append(np.ones(size=(num_out,1)))\n",
        "    # \n",
        "    # self.activations = []\n",
        "\n",
        "  def build_fnn(self,num_feat,num_hide,num_out,activns):\n",
        "#     assert len(num_hide)+1 == len(activns)\n",
        "     self.network = list()\n",
        "\n",
        "     lay_in = layer(num_feat,num_hide[0],activation=activns[0])\n",
        "     self.network.append(lay_in)\n",
        "     \n",
        "     for j in range(0,len(num_hide)-1):\n",
        "       self.network.append(layer(num_hide[j],num_hide[j+1],activation=activns[j+1]))\n",
        "       #pass\n",
        "\n",
        "     self.network.append(layer(num_hide[-1],num_out,activation=activns[-1]))\n",
        "     return 0\n",
        "\n",
        "  def add_layer(self,num_in,num_out,activn,is_out=False):\n",
        "    if(is_out): #if it is supposed to be new output layer\n",
        "      v = self.network[-1].shapeval[1]\n",
        "      assert v==num_in\n",
        "      self.network.append(layer(num_in,num_out,activation=activn))\n",
        "    \n",
        "    else: #num in and num out are just useless\n",
        "      vin = self.network[-2].shapeval[1]\n",
        "      vout = self.network[-1].shapeval[0]\n",
        "      self.network.append(layer(vin,vout,activation=activn))\n",
        "      v11 = self.network[-2]\n",
        "      self.network[-2] = self.network[-1]\n",
        "      self.network[-1] = v11\n",
        "\n",
        "  def loss(self,x,truev,loss_fn=\"mse\"):\n",
        "\n",
        "    if(loss_fn==\"mse\"):\n",
        "        return 0.5*np.mean(np.square(x-truev))\n",
        "    return 0\n",
        "  \n",
        "  def loss_der(self,x,truev,loss_fn=\"mse\"):\n",
        "    if(loss_fn==\"mse\"):\n",
        "      return np.mean((x-truev))\n",
        "    return 0\n",
        "    #pass\n",
        "\n",
        "  def fwd_pass(self,inp):\n",
        "    for lay in self.network:\n",
        "      inp = lay.after_apply_activ(inp)\n",
        "    return inp\n",
        "\n",
        "  def dropout(self,X, drop_probability):\n",
        "      keep_probability = 1 - drop_probability\n",
        "      mask = np.random.uniform(0, 1.0, X.shape) < keep_probability\n",
        "      scale = 0.0\n",
        "      if keep_probability > 0.0:\n",
        "          scale = (1/keep_probability)\n",
        "\n",
        "      return mask * X * scale\n",
        "\n",
        "  def backprop(self,inp,truev,eta,regularizer,loss_fn):\n",
        "    outp = self.fwd_pass(inp)\n",
        "\n",
        "    #backward pass: find diffvals\n",
        "    for i in range(len(self.network)):\n",
        "      lay_inx = len(self.network)-i-1\n",
        "      layr = self.network[lay_inx]\n",
        "\n",
        "      if(i==0): #implies last/op layer\n",
        "        layr.err = self.loss_der(outp,truev,loss_fn)\n",
        "        layr.diffval = (layr.err)*layr.activation_derivative(layr.activation_memo)\n",
        "\n",
        "      else:\n",
        "        layr_back = self.network[lay_inx+1]\n",
        "        layr.err = np.dot(layr_back.wts,layr_back.diffval)\n",
        "        layr.diffval = layr.err*layr.activation_derivative(layr.activation_memo)        \n",
        "      \n",
        "      self.network[lay_inx] = layr\n",
        "      \n",
        "      if regularizer is not None:\n",
        "        if(regularizer==\"L1\"):\n",
        "   #       print(layr.wts.shape)\n",
        "          lamd_v = 0.5\n",
        "          tempv = np.ones(shape=layr.wts.shape)\n",
        "          for val in range(layr.wts.shape[0]):\n",
        "            for v1 in range(len(layr.wts[val])):\n",
        "              if(layr.wts[val][v1]<0):\n",
        "                tempv[val][v1] = -1\n",
        "              if(layr.wts[val][v1]==0):\n",
        "                tempv[val][v1] = 0\n",
        "              \n",
        "          layr.penalty_norm = lamd_v*np.sum(tempv,axis=0)\n",
        "          del(tempv)\n",
        "\n",
        "        if(regularizer==\"L2\"):\n",
        "          lamd_v = 0.5\n",
        "        #  print(layr.wts.shape)\n",
        "          layr.penalty_norm = lamd_v*np.sum(layr.wts,axis=0)\n",
        "       #   print(layr.penalty_norm.shape)\n",
        "\n",
        "        if(regularizer==\"dropout\"):\n",
        "          for lr in self.network:\n",
        "            lr.wts = self.dropout(lr.wts,0.2)\n",
        "          layr.penalty_norm = None\n",
        "      \n",
        "      if layr.penalty_norm is not None:\n",
        "      #    print(layr.diffval.shape)\n",
        "      #    print(layr.penalty_norm.shape)\n",
        "          layr.diffval += layr.penalty_norm\n",
        "      \n",
        "      self.network[lay_inx] = layr\n",
        "\n",
        "    #fwd pass: weight update\n",
        "    for i in range(len(self.network)):\n",
        "      if(i!=0):\n",
        "        mtrx = np.atleast_2d(self.network[i-1].activation_memo)\n",
        "      else:\n",
        "        mtrx = np.atleast_2d(inp)\n",
        "\n",
        "      self.network[i].wts -= self.network[i].diffval*mtrx.T*eta\n",
        "\n",
        "  def predict(self,inpt,truev):\n",
        "    outp = self.fwd_pass(inpt)\n",
        "    if np.argmax(outp) == np.argmax(truev):\n",
        "      return True\n",
        "    #else:\n",
        "    return False\n",
        "\n",
        "  def find_acc(self,outp,truev):\n",
        "    all_v = 0\n",
        "    truevs = 0\n",
        "\n",
        "    for inx1 in range(len(outp)):\n",
        "      outpv = outp[inx1]\n",
        "      #print(outpv)\n",
        "      #print(truev)\n",
        "\n",
        "      truevv = truev[inx1]\n",
        "      \n",
        "      bn = self.predict(outpv,truevv)\n",
        "      if(bn):\n",
        "        truevs+=1\n",
        "      all_v +=1\n",
        "    print(truevs)\n",
        "    print(all_v)\n",
        "    return round(float(truevs/all_v),3)    \n",
        "\n",
        "  def train_fnn(self,testin,testtrue,inpt,truev,eta,maxiter,batch_size=32,regularizer=None,loss_fn=\"mse\"):\n",
        "    errs = []\n",
        "    tracc = []\n",
        "    testacc = []\n",
        "\n",
        "    if(batch_size!=0):\n",
        "      num_bins = int(len(inpt)/batch_size)\n",
        "    else:\n",
        "      num_bins = 1\n",
        "    eps = 10**-5\n",
        "    \n",
        "    for iter in range(maxiter):\n",
        "      err1tmp = []  \n",
        "      minb_inx = np.random.randint(0,num_bins)\n",
        "\n",
        "      if(batch_size!=0):\n",
        "        inxsl = minb_inx*batch_size\n",
        "        inpt1 = inpt[inxsl:] if minb_inx==num_bins-1 else inpt[inxsl:inxsl+batch_size]\n",
        "        truev1 = truev[inxsl:] if minb_inx==num_bins-1 else truev[inxsl:inxsl+batch_size]\n",
        "      else:\n",
        "        inpt1 = inpt\n",
        "        truev1 = truev\n",
        "\n",
        "      if(regularizer==\"batch\"):\n",
        "        mu1 = np.mean(inpt1)\n",
        "        sd1 = np.std(inpt1)\n",
        "        inpt1 = (inpt1-mu)/(sd1 + eps)\n",
        "      \n",
        "      for j in range(len(inpt1)):\n",
        "        self.backprop(inpt1[j],truev1[j],eta,regularizer,loss_fn)\n",
        "        err1tmp.append(self.loss(self.fwd_pass(inpt1[j]),truev1[j],loss_fn))\n",
        "\n",
        "        \n",
        "      errs.append(np.mean(np.asarray(err1tmp)))\n",
        "      print(err1tmp)\n",
        "      print(\"Epoch: %s, Loss: %f\" % (iter,errs[-1]))  \n",
        "      \n",
        "      if(iter%5==0):\n",
        "        tracc.append(self.find_acc(inpt,truev))\n",
        "        testacc.append(self.find_acc(testin,testtrue))\n",
        "\n",
        "    # checkpoints// commented\n",
        "    # with open(datapath+\"vals.pkl\",'wb') as pf:\n",
        "    #     pickle.dump(nn.network,pf,pickle.HIGHEST_PROTOCOL) wt saving so code can be resumed incase of BT\n",
        "\n",
        "    print(\"accuracy: %f\" %(self.find_acc(inpt,truev)))\n",
        "    \n",
        "    return errs,tracc,testacc\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpNqea5_Yg7G"
      },
      "source": [
        "# from sklearn.decomposition import PCA\n",
        "# pca = PCA(n_components=10, svd_solver='full')\n",
        "# pca.fit(traindata1.T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENubMxxqzbCp"
      },
      "source": [
        "nn = FNN()\n",
        "# nn.build_fnn(len(traindata1[0]),[800,500,250],ncls,[\"tanh\",\"sigmoid\",\"sigmoid\",\"tanh\",\"sigmoid\",\"sigmoid\"]) --> MNIST\n",
        "# nn.build_fnn(pca.components_,[10,10],ncls,[\"tanh\",\"sigmoid\",\"sigmoid\",\"tanh\",\"sigmoid\",\"sigmoid\"]) --> with pca if want to experiment\n",
        "nn.build_fnn(len(traindata1[0]),[800,500,100,50],ncls,[\"tanh\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\"]) #build and init weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLM_KMEizzNI"
      },
      "source": [
        "errs,tr,ts = nn.train_fnn(testdata1,testlabels1,traindata1,trainlabels1,10**-3,100,batch_size=0,regularizer=\"L2\") #0 batch size means no minibatches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNEVsCE4tmFG"
      },
      "source": [
        "# print(errs)\n",
        "# print(tr)\n",
        "# print(ts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wXfCY6htyys"
      },
      "source": [
        "# ----code for visualization - keep modifyig while generating plots\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.plot(range(epoch),errs,label=\"train\")\n",
        "# # plt.plot(range(0,100,10),ts,label=\"test\")\n",
        "# plt.xlabel(\"Epoch\")\n",
        "# plt.ylabel(\"Loss\")\n",
        "# plt.title(\"FNN, MSE loss, lr 10^-6\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}