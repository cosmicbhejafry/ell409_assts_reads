{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ELL409_A1_Q3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdhBtyDCjivF",
        "outputId": "0815ad5c-2aa7-4c35-c2d7-b8532a013115"
      },
      "source": [
        "!pip install numdifftools\n",
        "import numdifftools as ndt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numdifftools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/c0/b0d967160ecc8db52ae34e063937d85e8d386f140ad4826aae2086245a5e/numdifftools-0.9.39-py2.py3-none-any.whl (953kB)\n",
            "\r\u001b[K     |▍                               | 10kB 15.6MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 21.0MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 12.6MB/s eta 0:00:01\r\u001b[K     |█▍                              | 40kB 9.9MB/s eta 0:00:01\r\u001b[K     |█▊                              | 51kB 8.7MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 8.2MB/s eta 0:00:01\r\u001b[K     |██▍                             | 71kB 8.6MB/s eta 0:00:01\r\u001b[K     |██▊                             | 81kB 9.5MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 9.2MB/s eta 0:00:01\r\u001b[K     |███▍                            | 102kB 8.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 112kB 8.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 122kB 8.0MB/s eta 0:00:01\r\u001b[K     |████▌                           | 133kB 8.0MB/s eta 0:00:01\r\u001b[K     |████▉                           | 143kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 153kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 163kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 174kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 184kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 194kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 204kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 215kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 225kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 235kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 245kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 256kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 266kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 276kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 286kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 296kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 307kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 317kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 327kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 337kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 348kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 358kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 368kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 378kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 389kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 399kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 409kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 419kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 430kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 440kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 450kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 460kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 471kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 481kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 491kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 501kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 512kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 522kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 532kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 542kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 552kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 563kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 573kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 583kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 593kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 604kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 614kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 624kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 634kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 645kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 655kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 665kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 675kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 686kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 696kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 706kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 716kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 727kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 737kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 747kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 757kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 768kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 778kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 788kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 798kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 808kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 819kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 829kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 839kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 849kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 860kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 870kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 880kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 890kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 901kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 911kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 921kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 931kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 942kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 952kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 962kB 8.0MB/s \n",
            "\u001b[?25hInstalling collected packages: numdifftools\n",
            "Successfully installed numdifftools-0.9.39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji3CXYhNjq2m"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statistics as st\n",
        "import math\n",
        "#import PIL as pil\n",
        "from zipfile import ZipFile as zipf\n",
        "from PIL import Image as img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZL9I4zCkAOC",
        "outputId": "5d5940da-486d-48be-98de-961914cb9359"
      },
      "source": [
        "#Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drive_path = \"/content/drive/MyDrive/Assignment1_ELL409/Q3/Medical_MNIST.zip\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HargYjTzIuRk"
      },
      "source": [
        "def confusion_matrx(out_true_dict,num_class):\n",
        "  conf_m = np.zeros(dtype=int,shape=(num_class,num_class))\n",
        "  \n",
        "  for i in out_true_dict.keys():\n",
        "    true_v = i\n",
        "    pred_vs = out_true_dict[i]\n",
        "    for pred_v in pred_vs:\n",
        "      conf_m[true_v][pred_v] +=1\n",
        "\n",
        "  acc = 0\n",
        "  for i in range(num_class):\n",
        "    acc += conf_m[i]\n",
        "  acc = acc/sum(sum(conf_m))\n",
        "  return conf_m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IUP_rnCkfUk"
      },
      "source": [
        "categories = {\"AbdomenCT\":0,\"BreastMRI\":1,\"CXR\":2,\"ChestCT\":3,\"Hand\":4,\"HeadCT\":5}\n",
        "all_vals = {0:[],1:[],2:[],3:[],4:[],5:[]}\n",
        "\n",
        "with zipf(drive_path) as archive:\n",
        "  for entry in archive.infolist():\n",
        "    with archive.open(entry) as file:\n",
        "      key = file.name.split('/')[0]\n",
        "      imag = img.open(file)\n",
        "      #imag = imag.resize((32,32))\n",
        "      #print(imag.size, imag.mode, len(imag.getdata()))\n",
        "      #print()\n",
        "      all_vals[categories[key]].append(np.asarray(imag.getdata()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceJmDcBEcvg5",
        "outputId": "a58dac25-2ee8-4bd6-b67a-87230dc25dfe"
      },
      "source": [
        "lst = all_vals[0]\n",
        "k = int(len(lst)*0.2)\n",
        "train_set = lst[k:]\n",
        "#train_set.extend(lst[5*k:])\n",
        "test_set = lst[:k]\n",
        "del(lst)\n",
        "\n",
        "end_indxs = [(len(test_set)-1,len(train_set)-1)]\n",
        "\n",
        "for key in range(1,6):\n",
        "  lst = all_vals[key]\n",
        "  k = int(len(lst)*0.2)\n",
        "  \n",
        "  train_set.extend(lst[k:])\n",
        "  #train_set.extend(lst[5*k:])\n",
        "\n",
        "  test_set.extend(lst[:k])\n",
        "  end_indxs.append((len(test_set)-1,len(train_set)-1))\n",
        "  del(lst)\n",
        "\n",
        "print(end_indxs)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(1999, 7999), (3789, 15163), (5789, 23163), (7789, 31163), (9789, 39163), (11789, 47163)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO3m1A3Ef9Fi"
      },
      "source": [
        "del(all_vals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O68TxXSjb2m2"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train_set)\n",
        "train_set = scaler.transform(train_set)\n",
        "test_set = scaler.transform(test_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6lAeiiGfGrv",
        "outputId": "7b348147-d79a-4ca8-dcd0-54538be03b47"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "# Make an instance of the Model\n",
        "pca = PCA(n_components=3)\n",
        "pca.fit(train_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
              "    svd_solver='auto', tol=0.0, whiten=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKT9vDOUghYX"
      },
      "source": [
        "train_set = pca.transform(train_set)\n",
        "test_set = pca.transform(test_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nfg7Uu3DgsLc"
      },
      "source": [
        "traindict = {0:[],1:[],2:[],3:[],4:[],5:[]}\n",
        "testdict = {0:[],1:[],2:[],3:[],4:[],5:[]}\n",
        "testdict[0] = test_set[:end_indxs[0][0]+1]\n",
        "traindict[0] = train_set[:end_indxs[0][1]+1]\n",
        "\n",
        "for i in range(1,len(end_indxs)):\n",
        "  testdict[i] = test_set[end_indxs[i-1][0]+1:end_indxs[i][0]+1]\n",
        "  traindict[i] = train_set[end_indxs[i-1][1]+1:end_indxs[i][1]+1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eobgvK11kEK3"
      },
      "source": [
        "d = {\"train\":traindict,\"test\":testdict}\n",
        "import pickle\n",
        "allv_dir = \"/content/drive/MyDrive/Assignment1_ELL409/Q3/PCA_0test.pickle\"\n",
        "with open(allv_dir,\"wb\") as f:\n",
        "  pickle.dump(d,f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc71ji6FBSu_"
      },
      "source": [
        "import pickle\n",
        "allv_dir = \"/content/drive/MyDrive/Assignment1_ELL409/Q3/PCA_1test.pickle\"\n",
        "with open(allv_dir,\"rb\") as f:\n",
        "  datas = pickle.load(f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOxTZy5yCAQX"
      },
      "source": [
        "import pickle\n",
        "'''\n",
        "allv_dir = \"/content/drive/MyDrive/Assignment1_ELL409/Q3/all_medicalM_dict.pickle\"\n",
        "with open(allv_dir,\"wb\") as f:\n",
        "  pickle.dump(all_vals,f)\n",
        "\n",
        "data_splits = {0:[],1:[],2:[],3:[],4:[],5:[]}\n",
        "\n",
        "for key in all_vals.keys():\n",
        "  data = all_vals[key]\n",
        "  r = 0.2\n",
        "  len_1s = int(len(data)*r)\n",
        "  lst_tr = []\n",
        "  for i in range(5):\n",
        "    start = i*len_1s\n",
        "    end = (i+1)*len_1s\n",
        "    if(i==4):\n",
        "      end = len(data)\n",
        "\n",
        "    lst_tr.append(data[start:end])\n",
        "  data_splits[key] = lst_tr\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Assignment1_ELL409/Q3/32_split_medicalM_dict.pickle\",\"wb\") as f:\n",
        "  pickle.dump(data_splits,f)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuddT-xkO5Sc"
      },
      "source": [
        "'''\n",
        "allv_dir = \"/content/drive/MyDrive/Assignment1_ELL409/Q3/split_medicalM_dict.pickle\"\n",
        "\n",
        "with open(allv_dir,\"rb\") as f:\n",
        "  data_splits = pickle.load(f)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PP_Nz5L8PI-"
      },
      "source": [
        "'''\n",
        "def standardize_data(train_d,test_d,eps=10^-3):\n",
        "  mu_arr = np.mean(train_d,axis=0)\n",
        "  sd_arr = np.std(train_d,axis=0)\n",
        "  for val in range(len(sd_arr)):\n",
        "    if(sd_arr[val]==0):\n",
        "      sd_arr[val] = eps\n",
        "  \n",
        "  for val in range(len(train_d)):\n",
        "    lst_tr = train_d[val]\n",
        "    \n",
        "    lst_tr = np.subtract(lst_tr,mu_arr)\n",
        "    train_d[val] = np.divide(lst_tr,sd_arr)\n",
        "    \n",
        "  for val in range(len(test_d)):\n",
        "    lst_ts = test_d[val]\n",
        "    lst_ts = np.subtract(lst_ts,mu_arr)\n",
        "    test_d[val] = np.divide(lst_ts,sd_arr)\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  # Fit on training set only.\n",
        "  scaler.fit(train_img)\n",
        "  # Apply transform to both the training set and the test set.\n",
        "  train_img = scaler.transform(train_img)\n",
        "  test_img = scaler.transform(test_img)\n",
        "\n",
        "  return train_d,test_d\n",
        "\n",
        "def std_dicts(train_dict,test_dict):\n",
        "  for key in train_dict.keys():\n",
        "    train_dict[key],test_dict[key] = standardize_data(train_dict[key],test_dict[key])\n",
        "  return train_dict,test_dict\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGaLMv5QKh84"
      },
      "source": [
        "train_dict = {}\n",
        "test_dict = {}\n",
        "\n",
        "for k in data_splits.keys():\n",
        "  lst = data_splits[k]\n",
        "  test_dict[k] = lst[0]\n",
        "  val = lst[1]\n",
        "  for i in range(2,5):\n",
        "    val.append(lst[i])\n",
        "  train_dict[k] = val\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLyj0eELPtBW"
      },
      "source": [
        "train_dict,test_dict = std_dicts(train_dict,test_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2O-PJHSv8jR"
      },
      "source": [
        "import random\n",
        "\n",
        "def split_data(data, r,flag): \n",
        "    '''r: ratio of split, r indicates amount in train; flag indicates if random or linear split'''\n",
        "    train_len = int(len(data) * r) \n",
        "    test_len = len(data) - train_len\n",
        "    test = [] \n",
        "    train = []\n",
        "\n",
        "    if(flag==0):\n",
        "      indxs = random.sample(range(0, len(data)), test_len)\n",
        "      for v in range(len(data)):\n",
        "        if v in indxs:\n",
        "          test.append(data[v])\n",
        "        else:\n",
        "          train.append(data[v])\n",
        "\n",
        "    else:\n",
        "      train = data\n",
        "      while len(test) < test_len:\n",
        "        test.append(train.pop(0))\n",
        "\n",
        "    return train, test "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv8u2oNQt6-Y"
      },
      "source": [
        "traindict = {0:[],1:[],2:[],3:[],4:[],5:[]}\n",
        "testdict = {0:[],1:[],2:[],3:[],4:[],5:[]}\n",
        "\n",
        "for k in all_vals.keys():\n",
        "  lst = all_vals[k]\n",
        "  print(len(lst))\n",
        "  train,test = split_data(lst,0.8,0)\n",
        "  traindict[k] = np.asarray(train)\n",
        "  #testdict[k] = np.asarray(train[int(len(train)/2):])\n",
        "  testdict[k] = np.asarray(test)\n",
        "#print(traindict)\n",
        "#print(testdict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ngG5cMHBgxR"
      },
      "source": [
        "traindict = datas[\"train\"]\n",
        "testdict = datas[\"test\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATR6StYuyIDX"
      },
      "source": [
        "def MLE_Gaussian_Params(data_lst):\n",
        "  n = data_lst.shape[0] #number of data points\n",
        "  k = data_lst.shape[1] #feature vector dimension\n",
        "  mu = data_lst[0][:]\n",
        "  for j in range(1,n):\n",
        "    mu = mu + data_lst[j][:]\n",
        "  mu = mu/n\n",
        "  \n",
        "  sd = np.zeros(shape=(k,k),dtype=float)\n",
        "  #print(k)\n",
        "  for j in range(n):\n",
        "    tmp = data_lst[j][:] - mu\n",
        "    #print(np.transpose(tmp[np.newaxis]).shape)\n",
        "    sd += np.matmul(np.transpose(tmp[np.newaxis]),tmp[np.newaxis])\n",
        "  \n",
        "  sd = sd/n\n",
        "  \n",
        "  return mu,sd\n",
        "\n",
        "#@jit\n",
        "def PDF_normaldistr(inpt,mu,sd):\n",
        "  #mu = lst[0]\n",
        "  #sd = lst[1]\n",
        "  #print(\"in pdf\")\n",
        "  tmp = inpt-mu\n",
        "  print(tmp)\n",
        "  tmp = tmp[np.newaxis]\n",
        "  k = tmp.shape[1]\n",
        "  dt = np.linalg.det(sd)\n",
        "  if(dt!=0):\n",
        "    #print(\"det nonzero\")    \n",
        "    sdi = np.linalg.inv(sd)\n",
        "    expt = np.matmul(tmp,sdi)\n",
        "    expt = np.matmul(expt,np.transpose(tmp))*(-0.5)\n",
        "    expt = math.exp(expt[0][0])\n",
        "   # print(\"---\")\n",
        "   # print(expt)\n",
        "   # print(((2*math.pi)**(-0.5*k))*((dt)**(-0.5)))\n",
        "   # print(\"---\")\n",
        "    return expt*((2*math.pi)**(-0.5*k))*((dt)**(-0.5))\n",
        "  \n",
        "  else:\n",
        "    print(\"0 det\")\n",
        "    sd1 = sd + np.random.rand(sd.shape[0],sd.shape[1])*5\n",
        "    sdi = np.linalg.inv(sd1)\n",
        "    dt = np.linalg.det(sd1)\n",
        "    print(sdi)\n",
        "    print(dt)\n",
        "    expt = np.matmul(tmp,sdi)\n",
        "    expt = np.matmul(expt,np.transpose(tmp))*(-0.5)\n",
        "    print(expt)\n",
        "    expt = math.exp(expt[0])\n",
        "    return expt*((2*math.pi)**(-0.5*k))*((dt)**(-0.5))\n",
        "\n",
        "  #print(expt)\n",
        "\n",
        "def predict_label_MLEGauss(lst,lst_params,prior_arr):\n",
        "  out_lab = (-1,0)\n",
        "\n",
        "  for indx in range(len(lst_params)):\n",
        "    theta = lst_params[indx]\n",
        "    fx_y = PDF_normaldistr(lst,theta[0],theta[1])\n",
        "    fx_y = prior_arr[indx]*fx_y\n",
        "    if(out_lab[0]==-1):\n",
        "      out_lab = (fx_y,indx)\n",
        "    else:\n",
        "      if(out_lab[0]<fx_y):\n",
        "        out_lab = (fx_y,indx)\n",
        "\n",
        "  return out_lab[1]\n",
        "\n",
        "def Classify_MLE_Gauss(train_data_dict,test_data_dict,prior_arr):\n",
        "  lst_params = []\n",
        "  #true_class = []\n",
        "  for k in train_data_dict.keys():\n",
        "    lst_d = train_data_dict[k]\n",
        "    mu,sd = MLE_Gaussian_Params(lst_d)\n",
        "    lst_params.append((mu,sd))\n",
        "    #true_class.append(k)\n",
        "  \n",
        "  #print(lst_params)\n",
        "  \n",
        "  dict_true_out = {}\n",
        "  for ky in test_data_dict.keys():\n",
        "    lst_d = test_data_dict[ky]\n",
        "    out_list = []\n",
        "    for arr in lst_d:\n",
        "      out_l = predict_label_MLEGauss(arr,lst_params,prior_arr)\n",
        "      out_list.append(out_l)\n",
        "    \n",
        "    dict_true_out[ky] = out_list\n",
        "    \n",
        "  return dict_true_out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZT68WL5BiPx"
      },
      "source": [
        "mu = np.zeros(shape=(3,))\n",
        "sd = np.ones(shape=(3,3))\n",
        "PDF_normaldistr(np.asarray([0.5,0.5,1]),mu,sd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-9xuegHyOW8"
      },
      "source": [
        "prior_arr = [1/6,1/6,1/6,1/6,1/6,1/6]\n",
        "output = Classify_MLE_Gauss(traindict,testdict,prior_arr)\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDf7zu5CCU8H",
        "outputId": "400ad422-d43a-49dd-ee2e-7a818352870e"
      },
      "source": [
        "print(confusion_matrx(output,6))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2000    0    0    0    0    0]\n",
            " [   0 1790    0    0    0    0]\n",
            " [   0    0 1976    0   23    1]\n",
            " [   0    0    0 2000    0    0]\n",
            " [   2    1   60    0 1886   51]\n",
            " [   0  113    0    0   54 1833]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEODJqS_J1w1"
      },
      "source": [
        "Bayes with multivariate normal, data downsampled NOT PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBrdzNmJJ2U2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDCOM9AeJw2i"
      },
      "source": [
        "EM for GMMs,  Bayes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d_R22UkJy8z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eAbbmoVKMQj"
      },
      "source": [
        "Naive Bayes Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEqTHRkDKPi_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JDXbYZSKPLp"
      },
      "source": [
        "Expts with MAP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NnLbixkKTVo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxN50DKiKS8q"
      },
      "source": [
        "Expts with Parzen"
      ]
    }
  ]
}