{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Asst1_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGBWsl6BopxQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e292b82-8fd0-4f99-b831-a77451ad623b"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statistics as st\n",
        "import math\n",
        "!pip install numdifftools\n",
        "import numdifftools as ndt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numdifftools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/c0/b0d967160ecc8db52ae34e063937d85e8d386f140ad4826aae2086245a5e/numdifftools-0.9.39-py2.py3-none-any.whl (953kB)\n",
            "\r\u001b[K     |▍                               | 10kB 18.5MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 22.6MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 17.8MB/s eta 0:00:01\r\u001b[K     |█▍                              | 40kB 11.6MB/s eta 0:00:01\r\u001b[K     |█▊                              | 51kB 8.5MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 8.8MB/s eta 0:00:01\r\u001b[K     |██▍                             | 71kB 8.6MB/s eta 0:00:01\r\u001b[K     |██▊                             | 81kB 9.3MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 8.9MB/s eta 0:00:01\r\u001b[K     |███▍                            | 102kB 9.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 112kB 9.6MB/s eta 0:00:01\r\u001b[K     |████▏                           | 122kB 9.6MB/s eta 0:00:01\r\u001b[K     |████▌                           | 133kB 9.6MB/s eta 0:00:01\r\u001b[K     |████▉                           | 143kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 153kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 163kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 174kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 184kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 194kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 204kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 215kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 225kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 235kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 245kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 256kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 266kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 276kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 286kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 296kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 307kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 317kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 327kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 337kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 348kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 358kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 368kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 378kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 389kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 399kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 409kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 419kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 430kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 440kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 450kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 460kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 471kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 481kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 491kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 501kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 512kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 522kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 532kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 542kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 552kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 563kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 573kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 583kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 593kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 604kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 614kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 624kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 634kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 645kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 655kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 665kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 675kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 686kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 696kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 706kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 716kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 727kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 737kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 747kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 757kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 768kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 778kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 788kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 798kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 808kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 819kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 829kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 839kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 849kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 860kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 870kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 880kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 890kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 901kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 911kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 921kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 931kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 942kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 952kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 962kB 9.6MB/s \n",
            "\u001b[?25hInstalling collected packages: numdifftools\n",
            "Successfully installed numdifftools-0.9.39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoNPAS8cLsuA"
      },
      "source": [
        "# Loss functions\n",
        "# https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html\n",
        "# Define a list of loss fns and an array in ERM\n",
        "#http://www.ws.binghamton.edu/fowler/fowler%20personal%20page/EE522_files/EECE%20522%20Notes_26%20Ch_11B.pdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfEC_8twwk88"
      },
      "source": [
        "#Preprocessing\n",
        "\n",
        "def split_data(data, r): \n",
        "    train_len = int(len(data) * r) \n",
        "    test_len = len(data) - train_len\n",
        "    test = [] \n",
        "    train = list(data) \n",
        "    while len(test) < test_len:  \n",
        "        inx = random.randrange(train_len)  \n",
        "        test.append(train.pop(inx)) \n",
        "    return train, test \n",
        "\n",
        "def key_feat_map(data_arr):\n",
        "  dict1 = {}\n",
        "  for lst in data_arr:\n",
        "    k = lst[-1]\n",
        "    if k in dict1.keys():\n",
        "      dict1[k].append(lst[:-1])\n",
        "    else:\n",
        "      dict1[k] = [np.asarray(lst[:-1])]\n",
        "  for k in dict1:\n",
        "    dict1[k] = np.asarray(dict1[k])\n",
        "    #print(dict1[k].shape)\n",
        "  return dict1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgQUQqLdWN4l"
      },
      "source": [
        "#define precision, recall, fmeasure, accuracy, confusion matrix etc\n",
        "\n",
        "def confusion_matrx(out_true_dict,num_class):\n",
        "  conf_m = np.zeros(dtype=int,shape=(num_class,num_class))\n",
        "  \n",
        "  for i in out_true_dict.keys():\n",
        "    true_v = i\n",
        "    pred_vs = out_true_dict[i]\n",
        "    for pred_v in pred_vs:\n",
        "      conf_m[true_v][pred_v] +=1\n",
        "\n",
        "  acc = 0\n",
        "  for i in range(num_class):\n",
        "    acc += conf_m[i]\n",
        "  acc = acc/sum(sum(conf_m))\n",
        "  return conf_m\n",
        "\n",
        "def precis_recall_f(out_label,true_label,num_class):\n",
        "  cfm = confusion_matrx(out_label,true_label,2)\n",
        "  precision = (1/(cfm[1][1] + cfm[0][1]))*cfm[1][1]\n",
        "  recall = (1/(cfm[0][0] + cfm[1][0]))*cfm[0][0]\n",
        "  fscore = 2*precision*recall*(1/(precision+recall))\n",
        "  return precision,recall,fscore\n",
        "\n",
        "#http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.104.8244&rep=rep1&type=pdf\n",
        "#https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter13.pdf\n",
        "#https://dl.acm.org/doi/10.1023/A%3A1010920819831\n",
        "#http://mulan.sourceforge.net/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "Od-CdK_4Og7S",
        "outputId": "4fe4fdc2-6c1c-490d-b1fa-5be46ca7c196"
      },
      "source": [
        "x = np.ones(shape=(3,4))\n",
        "x[1][0] = 2.5\n",
        "x[1][2] = 3\n",
        "x[0][1] = 4.5\n",
        "x[0][3] = 5.6\n",
        "print(x)\n",
        "mu,sd=MLE_Gaussian_Params(x)\n",
        "PDF_normaldistr(x[0][:-1],mu,sd)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.  4.5 1.  5.6]\n",
            " [2.5 1.  3.  1. ]\n",
            " [1.  1.  1.  1. ]]\n",
            "4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-35d95993c7f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMLE_Gaussian_Params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mPDF_normaldistr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-feb48f5555be>\u001b[0m in \u001b[0;36mPDF_normaldistr\u001b[0;34m(inpt, mu, sd)\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;31m#mu = lst[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;31m#sd = lst[1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m   \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minpt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m   \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,) (4,) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leREpZdKN4pz"
      },
      "source": [
        "#MLE with Gaussian || Assumptions: class conditional density is gaussian, priors all equiprobable || Bayes classification\n",
        "#MLE => Loss function is KL-Divergence\n",
        "#http://www.cs.columbia.edu/~mcollins/em.pdf\n",
        "\n",
        "def MLE_Gaussian_Params(data_lst):\n",
        "  n = data_lst.shape[0] #number of data points\n",
        "  k = data_lst.shape[1] #feature vector dimension\n",
        "  mu = data_lst[0][:]\n",
        "  for j in range(1,n):\n",
        "    mu = mu + data_lst[j][:]\n",
        "  mu = mu/n\n",
        "  \n",
        "  sd = np.zeros(shape=(k,k),dtype=float)\n",
        "  print(k)\n",
        "  for j in range(n):\n",
        "    tmp = data_lst[j][:] - mu\n",
        "    #print(np.transpose(tmp[np.newaxis]).shape)\n",
        "    sd += np.matmul(np.transpose(tmp[np.newaxis]),tmp[np.newaxis])\n",
        "  \n",
        "  sd = sd/n\n",
        "  \n",
        "  return mu,sd\n",
        "\n",
        "def PDF_normaldistr(inpt,mu,sd):\n",
        "  #mu = lst[0]\n",
        "  #sd = lst[1]\n",
        "  tmp = inpt-mu\n",
        "  tmp = tmp[np.newaxis]\n",
        "  k = tmp.shape[1]\n",
        "  sdi = np.linalg.inv(sd)\n",
        "  expt = np.matmul(tmp,sdi)\n",
        "  expt = np.matmul(expt,np.transpose(tmp))*(-0.5)\n",
        "  expt = math.exp(expt[0][0])\n",
        "\n",
        "  return expt*((2*math.pi)**(-0.5*k))*((np.linalg.det(sd))**(-0.5))\n",
        "  #print(expt)\n",
        "\n",
        "def predict_label_MLEGauss(lst,lst_params,prior_arr = 1):\n",
        "  out_lab = (-1,0)\n",
        "\n",
        "  for indx in len(lst_params):\n",
        "    theta = lst_params[indx]\n",
        "    fx_y = PDF_normaldistr(lst,theta[0],theta[1])\n",
        "    fx_y = prior*fx_y\n",
        "    if(out_lab[0]==-1):\n",
        "      out_lab = (fx_y,indx)\n",
        "    else:\n",
        "      if(out_lab[0]<fx_y):\n",
        "        out_lab = (fx_y,indx)\n",
        "\n",
        "  return out_lab[1]\n",
        "\n",
        "def Classify_MLE_Gauss(train_data_dict,test_data_dict):\n",
        "  lst_params = []\n",
        "  #true_class = []\n",
        "  for k in train_data_dict.keys():\n",
        "    lst_d = train_data_dict[k]\n",
        "    mu,sd = MLE_Gaussian_Params(lst_d)\n",
        "    lst_params.append(tuple(mu,sd))\n",
        "    true_class.append(k)\n",
        "\n",
        "  dict_true_out = {}\n",
        "  for ky in test_data_dict.keys():\n",
        "    lst_d = test_data_dict[ky]\n",
        "    out_list = []\n",
        "    for arr in lst_d:\n",
        "      out_l = predict_label_MLEGauss(arr,lst_params)\n",
        "      out_list.append(out_l)\n",
        "    \n",
        "    dict_true_out[ky] = out_list\n",
        "    \n",
        "  return dict_true_out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7PjR7iFyFZo"
      },
      "source": [
        "#MLE for Naive Bayes || \n",
        "\n",
        "def MLE_uniGauss(data):\n",
        "  #data has to be single feature ka array\n",
        "  mu = np.sum(data)\n",
        "  mu = mu/data.shape[0]\n",
        "  \n",
        "  tmp = [(t - mu)^2 for t in data]\n",
        "  sd = (1/(data.shape[0]))*(sum(tmp))\n",
        "  return [mu,sqrt(sd)]\n",
        "\n",
        "def pdf_uniGauss(x,lst):\n",
        "  mu = lst[0]\n",
        "  sd = lst[1]\n",
        "  expt = (-0.5)*(x-mu)*(x-mu)*(1/sd)\n",
        "  return (1/sd)*(0.5)*(1/(math.pi))*math.exp(expt)\n",
        "\n",
        "def MLE_uniExp(data):\n",
        "  #data has to be single feature ka array\n",
        "  #would need to be modified to Laplacian agar neg features possible\n",
        "  mu = np.sum(data)\n",
        "  mu = mu/data.shape[0]\n",
        "  return [mu]\n",
        "\n",
        "def pdf_uniExp(x,mu1):\n",
        "  mu = mu1[0]\n",
        "  return (1/mu)*(math.exp(-x/mu))\n",
        "\n",
        "def MLE_uniUnf(data):\n",
        "  #data has to be single feature ka array\n",
        "  return [np.amax(data)]\n",
        "\n",
        "def pdf_uniUnf(x,mu):\n",
        "  return 1/mu[0]\n",
        "\n",
        "def MLE_predictLab_Naive(test,params_arr,fn_arr):\n",
        "  #all_vals = []\n",
        "  max_val = (-1,0)\n",
        "  for key in params_arr.keys():\n",
        "    params = params_arr[key]\n",
        "    val = 1\n",
        "    for i in len(range(test)):\n",
        "      pxi_y = fn_arr[i](test[i],params[i])\n",
        "      val = val*pxi_y\n",
        "    py = 1 #equiprobable for now\n",
        "    val = val*py\n",
        "    if(max_val[0]==-1):\n",
        "      max_val = (val,key)\n",
        "    else:\n",
        "      if(max_val[0]<val):\n",
        "        max_val = (val,key)\n",
        "\n",
        "  #  all_vals.append(tuple([val,key]))\n",
        "  print(\"boo\") \n",
        "  return max_val[1] \n",
        "\n",
        "def MLE_classify_Naive(train_dict,test_dict,MLE_arr,fn_arr,is_iid):\n",
        "  \n",
        "  #training\n",
        "  params_arr = {}\n",
        "  for key in train_dict.keys():\n",
        "    lst_d = train_dict[key]\n",
        "    len_feat = len(lst_d)\n",
        "    feat_params = [0 for t in range(len_feat)]\n",
        "    if(is_iid):\n",
        "      for i in range(len_feat):\n",
        "        xi = lst_d[:,i]\n",
        "        feat_params[i] = MLE_arr[i](xi)\n",
        "      print(\"fin if\")\n",
        "    else:\n",
        "      #build dict\n",
        "      MLE_dict = {}\n",
        "      for i in range(len(MLE_arr)):\n",
        "        key = MLE_arr[i]\n",
        "        if key in MLE_dict.keys():\n",
        "          MLE_dict[key].append(i)\n",
        "        else:\n",
        "          MLE_dict[key] = [i]\n",
        "      \n",
        "      for fn in MLE_dict.keys():\n",
        "        feats = MLE_dict[fn]\n",
        "        data_all = lst_d[feats[0]]\n",
        "        for j in range(1,len(feats)):\n",
        "          np.append(data_all,lst_d[feats[j]])\n",
        "        lst_params = fn(data_all)\n",
        "        for t in feats:\n",
        "          feat_params[t] = lst_params\n",
        "      print(\"fin els\")\n",
        "\n",
        "    params_arr[key] = feat_params  \n",
        "\n",
        "    #testing\n",
        "    dict_true_out = {}\n",
        "    for k in test_dict.keys():\n",
        "      lst_feats = test_dict[k]\n",
        "      out_labels = []\n",
        "      for feat in lst_feats:\n",
        "        lbl = MLE_predictLab_Naive(feat,params_arr,fn_arr)\n",
        "        out_labels.append(lbl)\n",
        "      dict_true_out[k] = out_labels\n",
        "    return dict_true_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wgd4qfWWxTe",
        "outputId": "2f88df7c-204f-4e20-efb1-e71b38415d6e"
      },
      "source": [
        "x = np.ones(shape=(3,10))\n",
        "np.cov(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJfblRAyZQaS"
      },
      "source": [
        "#MLE FOR GMMs\n",
        "#Expectation minimization algorithm\n",
        "\n",
        "def latent_probs(num_data,data,gmm_coeffs,gmm_mu,gmm_sd):\n",
        "  gamma_wts = np.zeros(shape=(num_data,len(gmm_coeffs)))\n",
        "  #normal_distrn = [,pdf_uniGauss]\n",
        "  #all_j_sums = []\n",
        "  for i in range(num_data):\n",
        "    temp_sum = 0\n",
        "    for j in range(len(gmm_coeffs)):\n",
        "      val = pdf_NormalDistr(data[i],gmm_mu[j],gmm_sd[j])\n",
        "      gamma_wts[i][j] = val*gmm_coeffs[j]\n",
        "      temp_sum += val*gmm_coeffs[j]\n",
        "    #all_j_sums.append(temp_sum)\n",
        "    gamma_wts[i,:] = gamma_wts[i,:]/temp_sum\n",
        "\n",
        "  return gamma_wts \n",
        "\n",
        "def find_maximized(data,gamma_wts):\n",
        "  gmm_mu = []\n",
        "  gmm_coeffs = []\n",
        "  gmm_sd = []\n",
        "  n = gamma_wts.shape[0]\n",
        "  \n",
        "  for k in range(gamma_wts.shape[1]):\n",
        "    pi_k = (1/n)*(np.sum(gamma_wts[:,k]))\n",
        "    mu_k = np.zeroes(shape=len(data[0]))\n",
        "    for i in range(n):\n",
        "      mu_k += [gamma_wts[i][k]*t for t in data[i]] ##might give bt with data[i] not iterable\n",
        "    mu_k = mu_k/(n*pi_k)\n",
        "\n",
        "    sd_k = np.zeroes(shape=(len(data[0]),len(data[0])))\n",
        "    for j in range(n):\n",
        "      tmp = data_lst[j] - mu_k\n",
        "      sd_k += np.multiply(np.matmul(np.transpose(tmp[np.newaxis]),tmp[np.newaxis]),gamma_wts[j][k])\n",
        "    \n",
        "    sd_k = sd_k/(n*pi_k)\n",
        "    \n",
        "    gmm_coeffs.append(pi_k)\n",
        "    gmm_mu.append(mu_k)\n",
        "    gmm_sd.append(sd_k)\n",
        "\n",
        "  return gmm_coeffs,gmm_mu,gmm_sd\n",
        "\n",
        "def logLikeli_EMM(data,gmm_coeffs,gmm_mu,gmm_sd,gamma_coeffs):\n",
        "  logl = 0\n",
        "  for i in range(len(data)):\n",
        "    for j in range(len(gmm_coeffs)):\n",
        "      val = math.log(gmm_coeffs[j]) + math.log(pdf_NormalDistr(data[i],gmm_mu[j],gmm_sd[j]))\n",
        "      val = val*gamma_coeffs[i][j]\n",
        "      logl+=val\n",
        "  return logl\n",
        "  print(\"bpp\")\n",
        "\n",
        "def EM_GMMs(n,data_lst,init_coeff,init_mu,init_sd,tolr=10^-5):\n",
        "  stopCon=False\n",
        "  \n",
        "  init_logL,temp_logL = 0,0\n",
        "\n",
        "  iter = 0\n",
        "  while(stopCon==False):\n",
        "    init_gm_wts = latent_probs(n,data_lst,init_coeff,init_mu,init_sd)\n",
        "    gmmc,gmmm,gmmsd = find_maximized(data_lst,init_gm_wts)\n",
        "    #init_gm_wts = latent_probs(n,data_lst,gmc,gmmm,gmmsd)\n",
        "    init_logL = temp_logL\n",
        "    temp_logL = logLikeli_EMM(data_lst,gmmc,gmmm,gmmsd,init_gm_wts)\n",
        "\n",
        "    iter+=1\n",
        "    if(abs(init_logL-templogL)<tolr and iter>1):\n",
        "      stopCon = True\n",
        "\n",
        "  print(\"bpp\")\n",
        "  return gmmc,gmmm,gmmsd\n",
        "\n",
        "#https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95\n",
        "\n",
        "def pdf_GMM(inpt,gmm_c,gmm_m,gmm_sd):\n",
        "  val = 0\n",
        "  for t in range(gmm_c):\n",
        "    val += gmm_c[t]*pdf_NormalDistr(inpt,gmm_m[t],gmm_sd[t])\n",
        "  return val\n",
        "\n",
        "def MLE_predictLab_GMM(inpt,params_arr):\n",
        "  #all_vals = []\n",
        "  max_val = (-1,0)\n",
        "  for key in params_arr.keys():\n",
        "    params = params_arr[key]\n",
        "    val = pdf_GMM(inpt,params[0],params[1],params[2])\n",
        "    py = 1 #equiprobable for now\n",
        "    val = val*py\n",
        "    if(max_val[0]==-1):\n",
        "      max_val = (val,key)\n",
        "    else:\n",
        "      if(max_val[0]<val):\n",
        "        max_val = (val,key)\n",
        "\n",
        "  #print(\"boo\") \n",
        "  return max_val[1] \n",
        "\n",
        "def MLE_Bayes_GMM(train_dict,test_dict,init_guess):\n",
        "  params_dict = {}\n",
        "  \n",
        "  for k in train_dict.keys():\n",
        "    c,mu,sd = EM_GMMs(len(train_dict[k]),train_dict[k],init_guess[0],init_guess[1],init_guess[2])\n",
        "    params_dict[k] = (c,mu,sd)\n",
        "  \n",
        "  tested_labels = {}\n",
        "  for k in test_dict.keys():\n",
        "    feats = test_dict[k]\n",
        "    out_labs = []\n",
        "    for feat in feats:\n",
        "      labl = MLE_predictLab_GMM(feat,params_dict)\n",
        "      out_labs.append(labl)\n",
        "    tested_labels[k] = out_labs\n",
        "  \n",
        "  return tested_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b31ZMxdXemfh"
      },
      "source": [
        "http://www.music.mcgill.ca/~ich/classes/mumt611_07/classifiers/ComparisonParzenStudy.pdf\n",
        "https://www.projectrhea.org/rhea/index.php/Lecture_16_-_Parzen_Window_Method_and_K-nearest_Neighbor_Density_Estimate_OldKiwi\n",
        "https://shapeofdata.wordpress.com/2013/07/23/gaussian-kernels/\n",
        "https://shapeofdata.wordpress.com/2013/04/23/nearest-neighbors-classification/\n",
        "https://stephens999.github.io/fiveMinuteStats/intro_to_em.html\n",
        "https://www.kth.se/social/upload/541152c5f27654548153cba3/04-problearn-2x2.pdf\n",
        "https://www.sciencedirect.com/science/article/pii/S0885064X08000526\n",
        "https://sebastianraschka.com/Articles/2014_kernel_density_est.html#23-the-window-function\n",
        "http://www.cs.cmu.edu/~aarti/Class/10701_Spring14/slides/MLE_MAP_Part1.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zikuve9RBZlR"
      },
      "source": [
        "#MAP of argmax f(X|Y,P)*f(P) || With Naive Bayes\n",
        "\n",
        "def MAP_expo_fXY_theta(data,theta_prior_param):\n",
        "  n = len(data)\n",
        "  return 1/((theta_prior_param)+sum(data))\n",
        "\n",
        "def MAP_gauss_fXY_theta_varfix(data,theta_prior_params,var):\n",
        "  n = len(data)\n",
        "  s = sum(data)\n",
        "  sm2 = theta_prior_params[1]**2\n",
        "  return (sm2*s + (var**2)*theta_prior_params[0])/(sm2*n+(var**2))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZJx14vZDggr"
      },
      "source": [
        "#https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html\n",
        "#https://numdifftools.readthedocs.io/en/latest/reference/generated/numdifftools.core.Gradient.html\n",
        "\n",
        "def grad_descent(fn,lr,init_guess,maxiter):\n",
        "  #opt_params = []\n",
        "  grad = ndt.Gradient(fn)\n",
        "  tmp_guess1 = init_guess\n",
        "  update_fn = lambda tmp: tmp - lr*grad(tmp)\n",
        "  stopCon = False\n",
        "  iter = 0\n",
        "  while(stopCon==False):\n",
        "    tmp_guess1 = update_fn(tmp_guess1)\n",
        "    iter+=1\n",
        "    if(iter>maxiter):\n",
        "      stopCon = True\n",
        "  return tmp_guess1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYFEkoA08c1r"
      },
      "source": [
        "#Parzen window est. \n",
        "def hypercube_fn(subs_vec,width=1):\n",
        "  for t in subs_vec:\n",
        "    if(np.abs(t)>(0.5*width)):\n",
        "      return 0\n",
        "  return 1\n",
        "  \n",
        "def gauss_kernel(subs_vec,width=1):\n",
        "  d = len(subs_vec)\n",
        "  expt = np.matmul(subs_vec[newaxis],np.transpose(subs_vec[newaxis]))\n",
        "  \n",
        "  return (width**d)*((math.pi)**(-0.5*d))*math.exp(expt*(-0.5/(width**2)))\n",
        "\n",
        "def parzen_label_est(inpt,train_dict,kernel,width=1):\n",
        "  #prior equiprobable\n",
        "  x=0\n",
        "  px_y = (-1,0)\n",
        "  for k in train_dict.keys():\n",
        "    train_pts = train_dict[k]\n",
        "    tmp_val = 0\n",
        "    for pt in train_pts:\n",
        "      in_vec = inpt - pt\n",
        "      tmp_val += kernel(in_vec,width)\n",
        "    tmp_val = tmp_val/(n*(width**d))\n",
        "    if(px_y[0]<tmp_val):\n",
        "      px_y = (tmp_val,k)\n",
        "  return px_y[1]\n",
        "\n",
        "def parzen_classify(train_dict,test_dict,kernel,width):\n",
        "  test_labs_dict = {}\n",
        "\n",
        "  for k in test_dict.keys():\n",
        "    datapts = test_dict[k]\n",
        "    out_lab = []\n",
        "    \n",
        "    for t in datapts:\n",
        "      out_lab.append(parzen_label_est(t,train_dict,kernel,width))\n",
        "\n",
        "    test_labs_dict[k] = out_lab\n",
        "  return test_labs_dict\n",
        "\n",
        "#https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\n",
        "#https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n",
        "#http://www.svcl.ucsd.edu/courses/ece271A/handouts/BayesIntro.pdf\n",
        "#file:///D:/!Sem5/ML/assignments/409_2020_Assignment_1.pdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gv6n1FTprXl-",
        "outputId": "e2bfe202-3ce8-45e0-dca6-f7fc80905e7a"
      },
      "source": [
        "x = np.zeros(shape=(2,))\n",
        "y = np.append(x,x)\n",
        "print(y)\n",
        "y = np.append(y,x)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dchtfhb-iPdf",
        "outputId": "15bf9d01-0a6c-44cc-bb09-f65783b8ae1e"
      },
      "source": [
        "x = [pdf_uniGauss,pdf_uniUnf,pdf_uniExp]\n",
        "#x[0]([1,2,3])\n",
        "dict1 = {}\n",
        "for i in x:\n",
        "  dict1[i] = 1\n",
        "print(dict1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{<function pdf_uniGauss at 0x7f708d3da400>: 1, <function pdf_uniUnf at 0x7f708d3da6a8>: 1, <function pdf_uniExp at 0x7f708d598c80>: 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d9-IIPDtrEd"
      },
      "source": [
        "# https://static1.squarespace.com/static/5ae0d0b48ab7227d232c2bea/t/5de5b403ae38e01033f5cf38/1575334918495/mle.pdf\n",
        "# http://www.cs.columbia.edu/~verma/classes/ml/index.html\n",
        "# https://stats.stackexchange.com/questions/112451/maximum-likelihood-estimation-mle-in-layman-terms\n",
        "# https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcD5NdV8kUPq"
      },
      "source": [
        "#MLE with GMMs || Assumptions: class conditional density is convex combination, priors all equiprobable || Naive Bayes assumption holds??\n",
        "#MLE => Loss function is KL-Divergence\n",
        "#this to be ignored\n",
        "'''\n",
        "def MLE_GMM_Params(data_lst):\n",
        "  n = data_lst.shape[0] #number of data points\n",
        "  k = data_lst.shape[1] #feature vector dimension\n",
        "  mu = data_lst[0][:]\n",
        "  for j in range(1,n):\n",
        "    mu = mu + data_lst[j][:]\n",
        "  mu = mu/n\n",
        "  \n",
        "  sd = np.zeros(shape=(k,k),dtype=float)\n",
        "  print(k)\n",
        "  for j in range(n):\n",
        "    tmp = data_lst[j][:] - mu\n",
        "    #print(np.transpose(tmp[np.newaxis]).shape)\n",
        "    sd += np.matmul(np.transpose(tmp[np.newaxis]),tmp[np.newaxis])\n",
        "  \n",
        "  sd = sd/n\n",
        "  \n",
        "  return mu,sd\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}